# Combined Python source files
# Root: .
# Generated: 2026-01-20T13:19:29+08:00

##### FILE: ./algorithms/base.py #####

# --------------------------------------------------------
# File: core/base.py
# Implement base classes for each algorithm, defining the 
# basic framework of the algorithms.
# --------------------------------------------------------

# The feature implementation is correct fine.

from abc import ABC, abstractmethod
from typing import List, Dict, Any
from core.task import Task

class BaseAlgorithm(ABC):
    """
    Abstract base class for all task allocation algorithms.

    This class defines the standard interface that the Agent class uses to
    interact with decision-making logic. Any specific algorithm implementation
    (e.g., Original PI, Dynamic PI) must inherit from this class.

    Attributes:
        agent_id (int): ID of the agent owning this algorithm instance.
        max_tasks (int): Maximum number of tasks the agent can handle.
        all_tasks (List[Task]): Reference to the global task list.
    """
    
    def __init__(self, agent_id: int, max_tasks: int, all_tasks: List[Task]) -> None:
        """
        Initializes the base algorithm state.

        Args:
            agent_id: The ID of the agent.
            max_tasks: Capacity constraint for the agent.
            all_tasks: List of all available tasks in the environment.
        """
        self.agent_id = agent_id
        self.max_tasks = max_tasks
        self.all_tasks = all_tasks

        # Derived classed should Initialize their specific data structures here
        # e.g., `significance_list`, `timestamp_list`, etc.


    @abstractmethod
    def bind_agent(self, agent: Any) -> None:
        """
        Binds the physical agent instance to the algorithm.
        """
        self.agent = agent

    @abstractmethod
    def finalize_setup(self, feasibility_matrix: List[List[bool]]) -> None:
        """
        Performs second-stage initialization with global info.

        Args:
            feasibility_matrix: A matrix indicating which tasks are feasible
                                for which agents.
        """
        pass

    @abstractmethod
    def run_iteration(self, messages: List[Dict[str, Any]]) -> None:
        """
        Executes one iteration of the algorithm (e.g., inclusion, consensus, removal).

        Args:
            messages: A list of messages received from neighboring agents.
        """
        pass

    @abstractmethod
    def get_plan(self) -> List[int]:
        """
        Retrieves the current task execution sequence decided by the algorithm.

        Returns:
            A list of task IDs representing the planned path.
        """
        pass 

    @abstractmethod
    def pack_message(self) -> Dict[str, Any]:
        """
        Packs the internal state into a message dictionary for broadcasting.

        Returns:
            A dictionary containing relevant algorithm state (e.g., bids,
            timestamps) to be sent to neighbors.
        """
        pass

    # --- Hooks for Dynamic Events (Optional for Static Algos) ---
    def on_task_completed(self, task_id: int) -> None:
        """
        Hook called when the agent physically completes a task.

        Args:
            task_id: The ID of the completed task.
        """
        pass

    def on_task_locked(self, task_id: int) -> None:
        """
        Hook called when the agent physically starts executing a task (locks it).

        Args:
            task_id: The ID of the task being locked.
        """
        pass

##### END FILE: ./algorithms/base.py #####

##### FILE: ./algorithms/dynamic_pi.py #####

# --------------------------------------------------------
# File: algorithms/dynamic_pi.py
#
# "Robust Dynamic PI for Communication-Constrained Environments"
#
# Key Academic Mechanisms:
#   1. Cyber-Physical Binding (Execution Locking): 
#      Resolves assignment-execution conflicts by prioritizing physical states.
#   2. Scalar Task Consensus (Lightweight Consistency): 
#      Replaces vector clocks with task-based scalar timestamps for robustness.
#   3. Inertia-Based Stability Control: 
#      Prevents "ping-pong" effects in dynamic topologies.
#   4. Tiered Spatio-Temporal Inclusion: 
#      Heuristically prioritizes tasks by urgency (deadline) then proximity.
# --------------------------------------------------------

import math
import numpy as np
from typing import List, Dict, Any, Tuple
from algorithms.base import BaseAlgorithm
from core.task import Task

# --- Constants Definitions ---
# Represents a task that is unassigned or has infinite cost
SIG_INFINITY = float('inf')   
# Represents a task that is physically being executed and cannot be preempted
SIG_HARD_LOCK = -1.0          

class DynamicPI(BaseAlgorithm):
    """
    Implements the Robust Dynamic PI Algorithm.

    This algorithm extends the Performance Impact (PI) method to handle 
    dynamic environments with restricted communication. It introduces a 
    Cyber-Physical state machine to handle execution conflicts and uses 
    scalar consensus for robust consistency.
    """

    def __init__(self, agent_id: int, max_tasks: int, all_tasks: List[Task]) -> None:
        """
        Initializes the Dynamic PI algorithm state.

        Args:
            agent_id: The unique ID of the agent.
            max_tasks: The maximum number of tasks this agent can carry.
            all_tasks: Reference list of all available tasks.
        """
        super().__init__(agent_id, max_tasks, all_tasks)
        num_tasks = len(all_tasks)
        
        # =========================================================
        # 1. Core Algorithm State
        # =========================================================
        # The planned execution sequence of task IDs
        self.tasks_sequence_list: List[int] = []
        # The significance list (Gamma).
        # Values can be:
        #   - SIG_HARD_LOCK: Physically locked by an agent
        #   - SIG_INFINITY: Unassigned
        #   - Float value: The estimated removal cost
        self.significance_list: List[float] = [SIG_INFINITY] * num_tasks
        # The assigned agent list
        self.assigned_agent_list: List[int] = [-1] * num_tasks 
        
        # =========================================================
        # 2. Consensus State
        # =========================================================
        # Scalar Timestamp List
        # Records the last update time for each task to resolve conflicts
        self.task_timestamps: List[float] = [0.0] * num_tasks
        
        # =========================================================
        # 3. Stability Control State
        # =========================================================
        # Cooling Blacklist: {task_id: expiration_time}
        # Prevents the agent from immediately re-bidding on a lost task
        self.blacklist: Dict[int, float] = {}
        
        # =========================================================
        # 4. Context Awareness & Binding
        # =========================================================
        self.current_time = 0.0
        self._is_currently_congested = False

        self.agent = None            # Bound in bind_agent()
        self.feasibility_matrix = [] # Bound in finalize_setup()

        # =========================================================
        # 5. Hyperparameters
        # =========================================================
        self.BLACKLIST_DURATION = 10.0      # Duration to ignore a lost task
        self.INERTIA_THRESHOLD = 15.0       # Min improvement needed to switch owners
        self.STALE_TIMEOUT = 30.0           # Time before resetting stale info
        self.CONGESTION_PENALTY = 2000.0    # Significance penalty during congestion
        self.CONGESTION_RADIUS = 20.0       # Radius to detect neighbor congestion

    # *******************************************************
    # Interface Implementation & State Mapping
    # *******************************************************
    
    def bind_agent(self, agent: Any) -> None:
        """Binds the physical agent instance to the algorithm."""
        self.agent = agent

    def finalize_setup(self, feasibility_matrix: List[List[bool]]) -> None:
        """Sets up the global feasibility matrix."""
        self.feasibility_matrix = feasibility_matrix
        
    def get_plan(self) -> List[int]:
        """Returns the current planned task sequence."""
        return self.tasks_sequence_list

    def pack_message(self) -> Dict[str, Any]:
        """
        Packs the internal state into a message for broadcasting.

        [Mechanism: Cyber-Physical Binding]
        If the agent is physically executing a task, it overrides the broadcast 
        significance to SIG_HARD_LOCK, regardless of internal calculations.

        Returns:
            A dictionary containing the agent's state, bids, and timestamps.
        """
        assert self.agent is not None, "Agent must be bound" 
       

        # 1. Copy the current Significance list
        broadcast_significance = list(self.significance_list)
        
        # 2. Enforce Hard Lock for physical consistency
        # If I am physically executing a task, I must broadcast a Hard Lock.
        if self.agent.status == 'EXECUTING' and self.agent.current_target_task_id is not None:
            target_task_id = self.agent.current_target_task_id
            broadcast_significance[target_task_id] = SIG_HARD_LOCK
            
            # Update timestamp to current time to ensure neighbors accept this lock
            self.task_timestamps[target_task_id] = self.current_time
        
        # 3. Construct message packet
        return {
            "sender_id": self.agent_id,
            "significance_list": broadcast_significance,
            "assigned_agent_list": self.assigned_agent_list,
            "task_timestamps": self.task_timestamps,
            "sender_position": self.agent.position
        }

    # *******************************************************
    # Cyber-Physical Event Hooks
    # *******************************************************

    def on_task_locked(self, task_id: int) -> None:
        """
        Triggered when the physical agent starts executing a task.
        
        [Mechanism: Execution Locking]
        Transitions the task state from 'Soft Assignment' (significance-based) to 
        'Hard Lock' (physical authority), preventing preemption.

        Args:
            task_id: The ID of the task being locked.
        """
        # 1. Force-set Hard Lock signal
        self.significance_list[task_id] = SIG_HARD_LOCK
        
        # 2. Confirm ownership locally
        self.assigned_agent_list[task_id] = self.agent_id
        
        # 3. Update timestamp to prioritize this state
        self.task_timestamps[task_id] = self.current_time

    def on_task_completed(self, task_id: int) -> None:
        """
        Triggered when the task is physically completed.
        
        Cleans up the internal state to prevent the algorithm from 
        re-assigning the completed task.

        Args:
            task_id: The ID of the completed task.
        """
        # 1. Remove from the execution queue
        if task_id in self.tasks_sequence_list:
            self.tasks_sequence_list.remove(task_id)
            
        # 2. Reset algorithm state for this task
        self.significance_list[task_id] = SIG_INFINITY
        self.assigned_agent_list[task_id] = -1

    # *******************************************************
    # Main Control Loop
    # *******************************************************

    def run_iteration(self, messages: List[Dict[str, Any]]) -> None:
        """
        Executes one iteration of the dynamic control loop.

        This follows a structured cycle: 
        Sense (Context) -> Maintain -> Perceive (Consensus) -> Plan (Removal/Inclusion).

        Args:
            messages: List of messages received from neighbors.
        """
        assert self.agent is not None, "Agent must be bound to run iteration" 
        self.current_time += 1.0

        # 1. Context Awareness: Check for local congestion
        self._is_currently_congested = self._calculate_congestion_status(messages)
        
        # 2. Maintenance: Clear stale information
        self._check_stale_information()
        
        # 3. Consensus Phase: Resolve conflicts using Scalar Consensus
        self._consensus_phase(messages)
        
        # 4. Planning Phase A: Task Removal
        self._task_removal_phase()
        
        # 5. Planning Phase B: Task Inclusion
        self._task_inclusion_phase()
        
        # 6. Cleanup: Manage cooling timers
        self._cleanup_blacklist()

    # *******************************************************
    # Phase 1: Consensus
    # *******************************************************

    def _consensus_phase(self, messages: List[Dict[str, Any]]) -> None:
        """
        Phase 1: Scalar Task Consensus.

        [Mechanism: Scalar Consensus with Inertia & Hard Locks]
        Resolves conflicts using task-based scalar timestamps instead of vector clocks.
        It prioritizes physical execution states (Hard Locks) and uses an inertia 
        threshold to prevent system oscillation ("ping-pong" effect).

        Args:
            messages: List of messages received from neighbors.
        """
        assert self.agent is not None

        for msg in messages:
            neighbor_significance = msg['significance_list']
            neighbor_assigned = msg['assigned_agent_list']
            neighbor_timestamps = msg['task_timestamps']

            for task_id in range(len(self.all_tasks)):
                # --- Rule 1: Self-Execution Authority ---
                # If I am physically executing the task, I am the ultimate authority.
                # I ignore neighbor info and refresh my timestamp.
                if self.agent.status == 'EXECUTING' and self.agent.current_target_task_id == task_id:
                    self.task_timestamps[task_id] = self.current_time
                    self.significance_list[task_id] = SIG_HARD_LOCK
                    self.assigned_agent_list[task_id] = self.agent_id
                    continue

                # --- Rule 2: Neighbor Hard Lock Authority ---
                # If neighbor signals a Hard Lock, they win immediately.
                if neighbor_significance[task_id] == SIG_HARD_LOCK:
                    self._accept_neighbor_info(task_id, neighbor_significance, neighbor_assigned, neighbor_timestamps)
                    
                    # If I thought I owned it, I lost it. Enter cooling period.
                    if self.assigned_agent_list[task_id] == self.agent_id:
                         self.blacklist[task_id] = self.current_time + self.BLACKLIST_DURATION
                    continue
                
                # --- Rule 3: Scalar Freshness & Inertia ---
                my_ts = self.task_timestamps[task_id]
                neighbor_ts = neighbor_timestamps[task_id]
                
                # Case A: Neighbor has strictly newer information
                if neighbor_ts > my_ts:
                    if self.assigned_agent_list[task_id] == self.agent_id:
                        # [Mechanism: Inertia]
                        # If I own the task, only switch if neighbor's significance (cost) 
                        # is significantly lower than mine (by INERTIA_THRESHOLD).
                        improvement = self.significance_list[task_id] - neighbor_significance[task_id]
                        if improvement > self.INERTIA_THRESHOLD:
                            self._accept_neighbor_info(task_id, neighbor_significance, neighbor_assigned, neighbor_timestamps)
                        else:
                            # Reject the switch, but update timestamp to acknowledge I saw the bid.
                            # This prevents the neighbor from repeatedly sending the same "new" info.
                            self.task_timestamps[task_id] = self.current_time
                    else:
                        # I am not the owner, simply accept the newer information.
                        self._accept_neighbor_info(task_id, neighbor_significance, neighbor_assigned, neighbor_timestamps)

                # Case B: Concurrent information (Approximate Equal timestamps)
                elif math.isclose(neighbor_ts, my_ts):
                    # Tie-Breaker: Lower Significance wins
                    if neighbor_significance[task_id] < self.significance_list[task_id]:
                         self._accept_neighbor_info(task_id, neighbor_significance, neighbor_assigned, neighbor_timestamps)
                    # Tie-Breaker: If Significance equal, Lower Agent ID wins
                    elif math.isclose(neighbor_significance[task_id], self.significance_list[task_id]):
                        if neighbor_assigned[task_id] != -1 and neighbor_assigned[task_id] < self.assigned_agent_list[task_id]:
                             self._accept_neighbor_info(task_id, neighbor_significance, neighbor_assigned, neighbor_timestamps)

    def _accept_neighbor_info(self, task_id, n_sigs, n_owners, n_timestamps):
        """Helper to atomically update local state with neighbor's info."""
        self.significance_list[task_id] = n_sigs[task_id]
        self.assigned_agent_list[task_id] = n_owners[task_id]
        self.task_timestamps[task_id] = n_timestamps[task_id]

    # *******************************************************
    # Phase 2: Removal 
    # *******************************************************

    def _task_removal_phase(self) -> None:
        """
        Phase 2: Task Removal.

        Removes tasks that were lost during consensus or have become 
        inefficient due to congestion or constraint violations.
        """
        assert self.agent is not None
        
        # 1. Remove tasks lost during Consensus (Ownership mismatch)
        lost_tasks = [
            task_id for task_id in self.tasks_sequence_list
            if self.assigned_agent_list[task_id] != self.agent_id
        ]
        for task_id in lost_tasks:
            self.tasks_sequence_list.remove(task_id)
            self.blacklist[task_id] = self.current_time + self.BLACKLIST_DURATION
            
        # 2. Iterative Greedy Removal (Synergy & Efficiency Check)
        while True:
            max_reduction = -SIG_INFINITY
            task_to_remove = -1
            
            # [Mechanism: Congestion Avoidance]
            # Reduce willingness to keep tasks if in a congested area.
            penalty = self.CONGESTION_PENALTY if self._is_currently_congested else 0.0
            
            for task_id in self.tasks_sequence_list:
                # Never remove the task currently being executed (Physical Hard Lock)
                if task_id == self.agent.current_target_task_id and self.agent.status == 'EXECUTING':
                    continue
                
                local_significance = self._calculate_significance(task_id)
                
                # Adjust local value with penalty
                adjusted_local_sig = local_significance + penalty 
                global_sig = self.significance_list[task_id]
                
                # Check 1: Constraint Violation (Deadline missed -> Infinite Cost)
                # This task will cause a timeout and needs to be removed
                if math.isinf(local_significance):
                    diff = SIG_INFINITY
                # Check 2: Efficiency (Am I still the best agent for this task?)
                # If (My Adjusted Cost) > (Market Cost), I am inefficient.
                else:
                    diff = adjusted_local_sig - global_sig

                if diff > max_reduction:
                    max_reduction = diff
                    task_to_remove = task_id
            
            # Execute removal if it improves the objective (reduces cost discrepancy)
            if max_reduction > 1e-6:
                self.tasks_sequence_list.remove(task_to_remove)
                # Note: No re-optimization call here as requested
            else:
                break

    # *******************************************************
    # Phase 3: Inclusion 
    # *******************************************************

    def _task_inclusion_phase(self) -> None:
        """
        Phase 3: Tiered Spatio-Temporal Inclusion.

        [Mechanism: Tiered Spatio-Temporal Inclusion]
        Heuristically selects tasks to add. It departs from pure greedy selection 
        by prioritizing tasks based on 'Urgency' (Deadline) first, and then 
        'Proximity' (Distance).
        """
        assert self.agent is not None
        
        # Capacity check
        if len(self.tasks_sequence_list) >= self.max_tasks:
            return

        # 1. Filter Candidates
        candidates = [
            task for task in self.all_tasks 
            if not task.completed 
            and task.id not in self.blacklist 
            and task.id not in self.tasks_sequence_list 
            and self.feasibility_matrix[self.agent_id][task.id]
        ]
        
        # 2. Tiered Sorting Strategy
        URGENCY_THRESHOLD = 150.0 
        
        def tiered_key(task):
            """
            Sorts tasks into two tiers:
            Tier 1 (Critical): Deadline is approaching. Sort by time remaining.
            Tier 2 (Normal): Deadline is far. Sort by distance (Spatial).
            """
            assert self.agent is not None
            time_left = task.deadline - self.current_time
            
            if time_left < URGENCY_THRESHOLD:
                return (0, time_left) 
            else:
                dist = np.linalg.norm(task.position - self.agent.position)
                return (1, dist)
        
        candidates.sort(key=tiered_key)
        
        # 3. Greedy Insertion
        for task in candidates:
            # Re-check capacity
            if len(self.tasks_sequence_list) >= self.max_tasks:
                break
                
            marginal_significance, pos = self._calculate_marginal_significance(task)
            
            # If infeasible (deadline violation), skip
            if math.isinf(marginal_significance):
                continue
                
            global_significance = self.significance_list[task.id]
            significance_gain = global_significance - marginal_significance
            
            # If positive gain, acquire the task
            if significance_gain > 0:
                self.tasks_sequence_list.insert(pos, task.id)
                self.assigned_agent_list[task.id] = self.agent_id
                self.significance_list[task.id] = marginal_significance
                
                # Claim ownership by updating timestamp to now
                self.task_timestamps[task.id] = self.current_time 
                
        # Update significance for all tasks after modifications
        self._update_internal_significance()

    def _update_internal_significance(self) -> None:
        """Batch updates significance for all tasks in the local sequence."""
        for task_id in self.tasks_sequence_list:
            self.significance_list[task_id] = self._calculate_significance(task_id)

    # *******************************************************
    # Helpers & Math
    # *******************************************************

    def _calculate_congestion_status(self, messages: List[Dict]) -> bool:
        """
        Detects local congestion based on neighbor proximity.
        
        Returns:
            True if a higher-priority agent (lower ID) is within 
            CONGESTION_RADIUS, prompting a yield behavior.
        """
        assert self.agent is not None
        my_pos = self.agent.position
        
        for msg in messages:
            neighbor_pos = msg.get('sender_position')
            sender_id = msg['sender_id']
            if neighbor_pos is not None:
                dist = np.linalg.norm(my_pos - neighbor_pos)
                if dist < self.CONGESTION_RADIUS and sender_id < self.agent_id:
                    return True
        return False

    def _check_stale_information(self) -> None:
        """
        Identifies and resets expired task information.
        
        Prevents "Ghost Tasks" (tasks owned by agents who have left the 
        network or crashed) from blocking assignment indefinitely.
        """
        for task_id in range(len(self.all_tasks)):
            if self.all_tasks[task_id].completed:
                continue
            if self.assigned_agent_list[task_id] == self.agent_id: 
                continue
            if self.assigned_agent_list[task_id] == -1: 
                continue
            
            # If info is older than threshold, reset to unassigned
            if (self.current_time - self.task_timestamps[task_id]) > self.STALE_TIMEOUT:
                self.assigned_agent_list[task_id] = -1
                self.significance_list[task_id] = SIG_INFINITY
                self.task_timestamps[task_id] = self.current_time

    def _cleanup_blacklist(self) -> None:
        """Removes expired entries from the blacklist."""
        expired_tasks = [tid for tid, expiry in self.blacklist.items() if self.current_time >= expiry]
        for tid in expired_tasks:
            del self.blacklist[tid]

    # --- Core Cost Calculations (Physical Layer) ---

    def _calculate_total_path_cost(self, path: List[int]) -> float:
        """
        Calculates the total time cost for a task sequence starting from 
        the agent's *current* dynamic position.
        
        Returns:
            Total time in seconds. Returns SIG_INFINITY if infeasible.
        """
        if self.agent is None: 
            return SIG_INFINITY
        cost = 0.0
        curr_pos = self.agent.position
        current_arrival_time = self.current_time 
        
        for task_id in path:
            task = self.all_tasks[task_id]
            dist = np.linalg.norm(task.position - curr_pos)
            travel_time = dist / self.agent.speed
            
            arrival_at_task = current_arrival_time + travel_time
            if arrival_at_task > task.deadline:
                return SIG_INFINITY
            
            cost += travel_time + task.exec_duration
            current_arrival_time = arrival_at_task + task.exec_duration
            curr_pos = task.position
        return cost

    def _calculate_significance(self, task_id: int) -> float:
        """
        Calculates the significance (marginal cost contribution) of a task.
        Significance = Cost(Path) - Cost(Path without task).
        """
        if task_id not in self.tasks_sequence_list: 
            return SIG_INFINITY
        
        cost_with = self._calculate_total_path_cost(self.tasks_sequence_list)
        if math.isinf(cost_with): 
            return SIG_INFINITY
            
        temp_sequence = self.tasks_sequence_list.copy()
        temp_sequence.remove(task_id)
        cost_without = self._calculate_total_path_cost(temp_sequence)
        
        return cost_with - cost_without

    def _calculate_marginal_significance(self, task: Task) -> Tuple[float, int]:
        """
        Calculates the marginal significance (insertion cost) of adding a new task.
        Returns the minimum cost increase and the best insertion index.
        """
        assert self.agent is not None

        min_marginal = SIG_INFINITY
        best_pos = -1
        base_cost = self._calculate_total_path_cost(self.tasks_sequence_list)
        
        if math.isinf(base_cost) and len(self.tasks_sequence_list) > 0:
             return SIG_INFINITY, -1
        
        for i in range(len(self.tasks_sequence_list) + 1):
            # [Hard Lock Protection] Cannot insert at index 0 if executing
            if i == 0 and self.agent.status == 'EXECUTING' and len(self.tasks_sequence_list) > 0:
                continue
            
            temp_sequence = self.tasks_sequence_list.copy()
            temp_sequence.insert(i, task.id)
            new_cost = self._calculate_total_path_cost(temp_sequence)
            
            if math.isinf(new_cost): 
                continue
                
            marginal = new_cost - base_cost
            if marginal < min_marginal:
                min_marginal = marginal
                best_pos = i
        return min_marginal, best_pos

##### END FILE: ./algorithms/dynamic_pi.py #####

##### FILE: ./algorithms/factory.py #####

# --------------------------------------------------------
# File: algorithms/factory.py
# Implement using the factory pattern, defining an 
# interface for creating various specific algorithms.
# --------------------------------------------------------

# The feature implementation is correct fine.

from algorithms.base import BaseAlgorithm
from algorithms.original_pi import OriginalPI
from algorithms.dynamic_pi import DynamicPI
from typing import List
from core.task import Task

class AlgorithmFactory:
    """
    Factory class to instantiate algorithms based on a string key.
    """

    @staticmethod
    def create(algo_name: str, agent_id: int, max_tasks: int, all_tasks: List[Task]) -> BaseAlgorithm:
        """
        Creates an instance of the requested algorithm strategy.

        Args:
            algo_name: Name key ('original', 'v1', 'v2', 'v3').
            agent_id: The agent's ID.
            max_tasks: Capacity constraint.
            all_tasks: Global task list.

        Returns:
            An initialized instance of a BaseAlgorithm subclass.
        """
        if algo_name == "original":
            return OriginalPI(agent_id, max_tasks, all_tasks)
        elif algo_name == "dynamic":
            return DynamicPI(agent_id, max_tasks, all_tasks)
        else:
            raise ValueError(f"Unknown algorithm name: {algo_name}")

##### END FILE: ./algorithms/factory.py #####

##### FILE: ./algorithms/__init__.py #####


##### END FILE: ./algorithms/__init__.py #####

##### FILE: ./algorithms/original_pi.py #####

# --------------------------------------------------------
# File: algorithms/original_pi.py
# Implemented PI-Avg. based on the paper.
# --------------------------------------------------------

import numpy as np
from typing import List, Dict, Any, Tuple
from algorithms.base import BaseAlgorithm
from core.agent import Agent
from core.task import Task

# Use positive infinity for initial cost values to represent unassigned states
INFINITY = float('inf')

class OriginalPI(BaseAlgorithm):
    """
    Implements the Original Performance Impact (PI) Algorithm.

    This class provides the core logic for the Performance Impact algorithm,
    strictly following the iterative three-phase procedure described in the paper.
    It focuses on minimizing the total global cost (time) while respecting
    temporal constraints (deadlines).

    Phases:
        1. Task Inclusion: Greedily add tasks that maximize marginal gain.
        2. Consensus: Resolve conflicts using CBBA rules (Consensus-Based Bundle Algorithm).
        3. Task Removal: Iteratively remove tasks that are outbid or become inefficient.

    References:
        Zhao et al., "A Heuristic Distributed Task Allocation Method for Multivehicle
        Multitask Problems and Its Application to Search and Rescue Scenario",
        IEEE Transactions on Cybernetics, 2016.
    """

    def __init__(self, agent_id: int, max_tasks: int, all_tasks: List[Task]) -> None:
        """Initializes the PI algorithm state.

        Args:
            agent_id (int): The unique ID of the agent running this algorithm.
            max_tasks (int): The maximum number of tasks this agent can carry (capacity).
            all_tasks (List[Task]): A reference list of all available tasks in the environment.
        """
        super().__init__(agent_id, max_tasks, all_tasks)
        num_tasks = len(all_tasks)
        
        # --------- 1. PI Algorithm State ---------
        # a: The current sequence of tasks assigned to this agent (ordered list)
        self.tasks_sequence_list: List[int] = []
        # γ: The significance list
        self.significance_list: List[float] = [INFINITY] * num_tasks
        # β: The assigned list
        self.assigned_agent_list: List[int] = [-1] * num_tasks
        
        # --------- 2. Consensus State ---------
        # δ: the timestamp list
        self.timestamp_list: List[float] = [] 

        # Reference to the physical agent
        self.agent: Any = None
        # Iteration step
        self.current_time = 0.0
        
    def bind_agent(self, agent: Agent):
        """
        Binds the physical agent instance to the algorithm.

        Args:
            agent: The physical Agent instance.
        """
        self.agent = agent
    
    def finalize_setup(self, feasibility_matrix: List[List[bool]]) -> None:
        """
        Performs second-stage initialization with global information.

        Args:
            feasibility_matrix: A boolean matrix where M[i][j] indicates if 
                                agent i can execute task j.
        """
        self.feasibility_matrix = feasibility_matrix
        num_agents = len(feasibility_matrix)
        # Initialize vector clock with zeros for all known agents
        self.timestamp_list = [0.0] * num_agents

    def get_plan(self) -> List[int]:
        """Returns the current planned sequence of task IDs."""
        return self.tasks_sequence_list

    def pack_message(self) -> Dict[str, Any]:
        """
        Packs the internal state into a dictionary for broadcasting.

        Returns:
            A dictionary containing:
                - sender_id: ID of this agent.
                - significance_list: The list of bids/costs.
                - assigned_agent_list: The list of task owners.
                - timestamp_list: The vector clock for consensus.
        """
        return {
            "sender_id": self.agent_id,
            "significance_list": self.significance_list,
            "assigned_agent_list": self.assigned_agent_list,
            "timestamp_list": self.timestamp_list
        }

    def on_task_completed(self, task_id: int) -> None:
        """
        Callback handler for when a task is physically completed.

        Args:
            task_id: The ID of the completed task.
        """
        if task_id in self.tasks_sequence_list:
            self.tasks_sequence_list.remove(task_id)

    def run_iteration(self, messages: List[Dict[str, Any]]) -> None:
        """
        Executes one complete iteration of the PI algorithm.

        The iteration consists of three phases:
            1. Inclusion: greedily add tasks that reduce local cost.
            2. Consensus: resolve conflicts with neighbors based on bids.
            3. Removal: remove tasks that are no longer valid or optimal.

        Args:
            messages: A list of messages received from neighboring agents.
        """
        # --------- Phase 1: Tasks Inclusion ---------
        self._task_inclusion_phase()
        
        # --------- Phase 2: Information Consensus ---------
        self._consensus_phase(messages)
        
        # --------- Phase 3: Tasks Removal ---------
        self._task_removal_phase()

        # Update logical clock at the end of the iteration
        self.current_time += 1.0
        if self.timestamp_list:
            self.timestamp_list[self.agent_id] = self.current_time

    # ************************************************************** 
    # Phase Implementations
    # ************************************************************** 
    def _task_inclusion_phase(self) -> None:
        """
        Phase 1: Greedily adds tasks to the bundle.
        
        The agent iteratively selects the unassigned task that 
        maximizes the difference between the current global significance
        and the agent's marginal significance 
        """
        while len(self.tasks_sequence_list) < self.max_tasks:
            max_significance_gain = 0.0
            best_task_id = -1
            best_insertion_index = -1

            # Iterate through each task to find the best candidate
            for task in self.all_tasks:
                # Skip if already in my local schedule
                if task.id in self.tasks_sequence_list: 
                    continue

                # Skip if I cannot physically perform this task type
                if not self.feasibility_matrix[self.agent_id][task.id]: 
                    continue

                # Calculate the marginal significance if we add this task
                marginal_significance, position = self._calculate_marginal_significance(task)
                
                # If the task cannot be added due to constraints, skip it
                if marginal_significance == INFINITY:
                    continue

                # Calculate potential gain: (Current Global Significance - My Marginal Significance)
                current_global_significance = self.significance_list[task.id]
                gain = current_global_significance - marginal_significance

                # Track the task that offers the highest gain
                if gain > max_significance_gain:
                    max_significance_gain = gain
                    best_task_id = task.id
                    best_insertion_index = position

            # If a profitable task is found, add it to the sequence
            if max_significance_gain > 0 and best_task_id != -1:
                self.tasks_sequence_list.insert(best_insertion_index, best_task_id)
                
                # Update internal state for the new task
                self.significance_list[best_task_id] = self._calculate_significance(best_task_id)
                self.assigned_agent_list[best_task_id] = self.agent_id
            else:
                # Stop if no task improves the objective function
                break

        # Cascade Update: Adding a task changes the position and costs of 
        # other tasks in the sequence. Update significance for all owned tasks.
        self._update_internal_significance()

    def _consensus_phase(self, messages: List[Dict]):
        """
        Phase 2: Information Consensus using CBBA Rules.

        Resolves conflicts based on the Consensus-Based Bundle Algorithm decision rules.
        It handles information staleness using vector clocks and determines winning bids
        based on minimizing significance (cost).

        Args:
            messages: A list of message dictionaries from neighbors.
        """
        for msg in messages:
            sender_id = msg['sender_id']
            neighbor_y = msg['significance_list']
            neighbor_z = msg['assigned_agent_list']
            neighbor_s = msg['timestamp_list']

            # Iterate over every task to resolve potential conflicts
            for j in range(len(self.all_tasks)):
                # --- Aliases for CBBA logic readability ---
                k = sender_id               # Sender
                i = self.agent_id           # Self

                # State values
                z_kj = neighbor_z[j]                # Who k thinks owns j
                z_ij = self.assigned_agent_list[j]  # Who i thinks owns j
                y_kj = neighbor_y[j]                # k's cost for j
                y_ij = self.significance_list[j]    # i's cost for j

                # Timestamps
                # s_km: k's timestamp about agent m
                # s_im: i's timestamp about agent m

                action = 'LEAVE' # Default

                # --- Helper: Comparison Logic ---
                def is_neighbor_better(n_cost, my_cost, n_id, my_id):
                    """Returns True if neighbor has lower cost, or ties with lower ID."""
                    if n_cost < my_cost: 
                        return True
                    if abs(n_cost - my_cost) < 1e-6 and n_id < my_id: 
                        return True # Tie-breaker
                    return False

                # --- CBBA Decision Table Implementation ---
                # Case A: Sender thinks Sender (k) owns it
                if z_kj == k:
                    # 1. I think I (i) own it -> Direct Conflict
                    if z_ij == i:
                        if is_neighbor_better(y_kj, y_ij, k, i):
                            action = 'UPDATE'
                    # 2. I think Sender (k) owns it -> Update bid if changed
                    elif z_ij == k:
                        action = 'UPDATE'
                    # 3. I think it's unassigned -> Update
                    elif z_ij == -1:
                        action = 'UPDATE'
                    # 4. I think a third party (m) owns it
                    else:
                        m = z_ij
                        s_km = neighbor_s[m]
                        s_im = self.timestamp_list[m]
                        # Update if sender has newer info about m OR sender beats m's bid
                        if s_km > s_im or is_neighbor_better(y_kj, y_ij, k, m):
                            action = 'UPDATE'

                # Case B: Sender thinks I (i) own it
                elif z_kj == i:
                    if z_ij == i:
                        action = 'LEAVE'
                    elif z_ij == k:
                        # Sender says I have it, I say Sender has it -> Sender is confused or I am stale
                        action = 'RESET'
                    elif z_ij == -1:
                        action = 'LEAVE'
                    else: # z_ij == m
                        m = z_ij
                        # s_km > s_im
                        if neighbor_s[m] > self.timestamp_list[m]:
                            action = 'RESET' # Sender knows m is out

                # Case C: Sender thinks it's Empty (-1)
                elif z_kj == -1:
                    if z_ij == i:
                        action = 'LEAVE'
                    elif z_ij == k:
                        action = 'UPDATE' # Sender released it
                    elif z_ij == -1:
                        action = 'LEAVE'
                    else: # z_ij == m
                        m = z_ij
                        # s_km > s_im
                        if neighbor_s[m] > self.timestamp_list[m]:
                            action = 'UPDATE' # Sender knows m is out

                # Case D: Sender thinks Other (m) owns it
                else: # z_kj == m
                    m = z_kj
                    if z_ij == i:
                        # s_km > s_im
                        if neighbor_s[m] > self.timestamp_list[m] and is_neighbor_better(y_kj, y_ij, m, i):
                            action = 'UPDATE'
                    elif z_ij == k:
                        # s_km > s_im
                        if neighbor_s[m] > self.timestamp_list[m]:
                            action = 'UPDATE'
                        else:
                            action = 'RESET'
                    elif z_ij == -1:
                        # s_km > s_im
                        if neighbor_s[m] > self.timestamp_list[m]:
                            action = 'UPDATE'
                    elif z_ij == m:
                        # s_km > s_im
                        if neighbor_s[m] > self.timestamp_list[m]:
                            action = 'UPDATE'
                    else: # z_ij == n 
                        n = z_ij
                        s_km = neighbor_s[m] # Sender's knowledge of m
                        s_im = self.timestamp_list[m] # My knowledge of m
                        s_kn = neighbor_s[n] # Sender's knowledge of n
                        s_in = self.timestamp_list[n] # My knowledge of n

                        # Logic from CBBA Table Col 4 Row 4
                        if s_km > s_im and s_kn > s_in:
                            action = 'UPDATE'
                        elif s_km > s_im and is_neighbor_better(y_kj, y_ij, m, n):
                            action = 'UPDATE'
                        elif s_kn > s_in and s_im > s_km:
                            action = 'RESET'
                        else:
                            action = 'LEAVE'

                # --- Execute Action ---
                if action == 'UPDATE':
                    self.significance_list[j] = y_kj
                    self.assigned_agent_list[j] = z_kj
                elif action == 'RESET':
                    self.significance_list[j] = INFINITY
                    self.assigned_agent_list[j] = -1

            for k_idx in range(len(self.timestamp_list)):
                self.timestamp_list[k_idx] = max(self.timestamp_list[k_idx], neighbor_s[k_idx])

    def _task_removal_phase(self) -> None:
        """
        Phase 3: Iterative Greedy Task Removal.

        Identifies tasks that should be removed because:
        1. The agent lost ownership during consensus (outbid).
        2. The task is no longer efficient to keep (constraint violation or synergy loss).
        """
        # 1. Identify tasks that are definitely lost (ownership changed externally)
        tasks_to_check_for_removal = {
            task_id
            for task_id in self.tasks_sequence_list
            if self.assigned_agent_list[task_id] != self.agent_id
        }

        # 2. Iteratively remove tasks until the set is clean
        while len(tasks_to_check_for_removal) > 0:
            max_reduction = -INFINITY
            task_to_remove = -1

            # Find the task whose removal maximizes the reduction in cost discrepancy
            for tid in tasks_to_check_for_removal:
                local_significance = self._calculate_significance(tid)
                global_significance = self.significance_list[tid]

                # Difference between what I pay (local) and what the market pays (global)
                # If local > global, I am inefficient and should consider removing.
                reduction = local_significance - global_significance

                if reduction > max_reduction:
                    max_reduction = reduction
                    task_to_remove = tid
            
            # Execute removal if it helps alignment with global state
            if max_reduction > 0:
                self.tasks_sequence_list.remove(task_to_remove)
                tasks_to_check_for_removal.remove(task_to_remove)
            else:
                # If no more removals improve the situation, stop.
                break 

        # 3. Re-confirm ownership of remaining tasks
        # Any task still in my list is mine, and I update its significance based on the new path.
        for tid in self.tasks_sequence_list:
            if self.assigned_agent_list[tid] != self.agent_id:
                self.assigned_agent_list[tid] = self.agent_id
                self.significance_list[tid] = self._calculate_significance(tid)

    # ************************************************************** 
    # Math Helpers (Cost Calculation)
    # ************************************************************** 
    def _calculate_total_path_cost(self, path: List[int]) -> float:
        """
        Calculates the total time cost (travel + execution) for a task sequence.

        Args:
            sequence: A list of task IDs.

        Returns:
            Total time in seconds. Returns INFINITY if deadlines are violated.
        """
        # Safety check: Ensure agent is bound
        if not self.agent:
            return INFINITY

        cost = 0.0
        curr_pos = self.agent.position
        for tid in path:
            task = self.all_tasks[tid]
            dist = np.linalg.norm(task.position - curr_pos)
            cost += dist / self.agent.speed + task.exec_duration
            curr_pos = task.position
        return cost

    def _calculate_significance(self, task_id) -> float:
        """
        Calculates the significance (marginal cost contribution) of a task.

        Significance is defined as: Cost(Path) - Cost(Path without task).

        Args:
            task: The task object to evaluate.

        Returns:
            The marginal cost value. Returns INFINITY if task is not in the list.
        """
        if task_id not in self.tasks_sequence_list: 
            return 0.0

        # 1. Calculates current total cost (baseline cost)
        base_cost = self._calculate_total_path_cost(self.tasks_sequence_list)

        # 2. Simulate the task chain after removal
        temp_sequence = self.tasks_sequence_list.copy()
        temp_sequence.remove(task_id)

        # 3. Calculate the total cost after removal
        reduced_cost = self._calculate_total_path_cost(temp_sequence)

        return base_cost - reduced_cost

    def _calculate_marginal_significance(self, task: Task) -> Tuple[float, int]:
        """
        Finds the best insertion position and cost for a new task.

        Iterates through all possible insertion points in the current path
        to find the one that minimizes the increase in total cost.

        Args:
            task: The new task to consider adding.

        Returns:
            A tuple (min_marginal_cost, best_insertion_index).
        """
        min_marginal_cost = INFINITY
        best_position = -1

        # Calculates the execution cost of the current task sequence (baseline cost)
        base_cost = self._calculate_total_path_cost(self.tasks_sequence_list)
        
        # Iterates through all possible insertion points in the current path
        for i in range(len(self.tasks_sequence_list) + 1):
            temp_task_sequence = self.tasks_sequence_list.copy()
            temp_task_sequence.insert(i, task.id)

            # Check temporal constraints
            if not self._is_path_valid(temp_task_sequence):
                continue

            # Calculate the cost after insertion
            new_cost = self._calculate_total_path_cost(temp_task_sequence)
            marginal_cost = new_cost - base_cost
            if marginal_cost < min_marginal_cost:
                min_marginal_cost = marginal_cost
                best_position = i
        return min_marginal_cost, best_position

    def _calculate_reach_time(self, task_id: int, pos: int) -> float:
        """
        Calculates the arrival time at a specific task.

        Used to check deadline constraints.

        Args:
            task_id: The ID of the target task.
            pos: The proposed insertion index in the path.

        Returns:
            The simulation time when the agent would arrive at the task.
        """
        # Safety check: Ensure agent is bound
        if not self.agent:
            return INFINITY

        cost = 0.0
        curr_pos = self.agent.position

        # Calculate time to traverse the path up to the insertion point
        for tid in self.tasks_sequence_list[:pos]:
            task = self.all_tasks[tid]
            cost += np.linalg.norm(task.position - curr_pos) / self.agent.speed + task.exec_duration
            curr_pos = task.position

        # Add travel time to the target task itself
        target = self.all_tasks[task_id]
        return cost + np.linalg.norm(target.position - curr_pos) / self.agent.speed

    def _is_path_valid(self, sequence: List[int]) -> bool:
        """
        Checks if a task sequence satisfies all temporal constraints (deadlines).

        Args:
            sequence: List of task IDs.

        Returns:
            True if feasible, False otherwise.
        """
        current_time = self.current_time
        current_position = self.agent.position
        
        for task_id in sequence:
            task = self.all_tasks[task_id]
            
            # 1. Calculate the time required to reach the task (flight time)
            dist = np.linalg.norm(task.position - current_position)
            travel_time = dist / self.agent.speed
            
            # 2. Update estimated arrival time
            arrival_time = current_time + travel_time
            
            # 3. Critical check: does the task exceed its deadline?
            #    If any single task in the chain misses its deadline, the entire chain becomes invalid
            if arrival_time > task.deadline:
                return False
            
            # 4. Update current time and position for evaluating the next task
            #    (Task execution itself also requires duration `exec_duration`)
            current_time = arrival_time + task.exec_duration
            current_position = task.position
            
        return True

    def _update_internal_significance(self):
        """Recalculates significance values for all tasks in the local bundle."""
        for tid in self.tasks_sequence_list:
            self.significance_list[tid] = self._calculate_significance(tid)

##### END FILE: ./algorithms/original_pi.py #####

##### FILE: ./core/agent.py #####

# --------------------------------------------------------
# File: core/agent.py
# Implemented the Agent entity in the simulation, 
# where each Agent is equipped with a decision-making 
# module for task assignment.
# --------------------------------------------------------

# The feature implementation is correct fine.

import numpy as np
from typing import Dict, List
from algorithms.base import BaseAlgorithm
from core.task import Task

# Define agent type constants
AGENT_TYPE_MEDICINE = 'medicine'
AGENT_TYPE_FOOD     = 'food'

class Agent:
    """
    Represents an agent in the simulation.

    The Agent is responsible for physical movement, state maintenance, and
    delegating decision-making to an injected algorithm strategy.

    Attributes:
        id (int): Unique identifier.
        agent_type (str): Type of the agent (e.g., 'medicine', 'food').
        position (np.ndarray): Current (x, y) coordinates.
        speed (float): Movement speed in units per second.
        color (Tuple): RGB color for visualization.
        algorithm (BaseAlgorithm): The decision-making strategy instance.
        status (str): Current physical status ('IDLE', 'MOVING', 'EXECUTING').
        completed_tasks_log (List[int]): History of completed task IDs.
    """

    def __init__(self, id: int, agent_type: str, position: tuple, speed: float,
                 color: tuple, algorithm: BaseAlgorithm) -> None:
        """
        Initializes the Agent.

        Args:
            id: Agent ID.
            agent_type: Type string.
            position: Initial (x, y) tuple.
            speed: Movement speed.
            color: Visualization color.
            algorithm: An instance of a subclass of BaseAlgorithm.
        """
        # Basic attribute
        self.id = id
        self.agent_type = agent_type
        self.position = np.array(position, dtype=float)
        self.speed = speed
        
        # Decision-Making module
        self.algorithm = algorithm
        self.algorithm.bind_agent(self)

        # Physical state
        self.color = color
        self.status = 'IDLE'
        self.current_target_task_id: int | None = None
        self.time_at_task = 0.0
        self.completed_tasks_log: List[int] = []

        # Statistic distance
        self.total_distance: float = 0.0

    def __repr__(self) -> str:
        return f"Agent(id={self.id}, type={self.agent_type}, pos={self.position})"

    def update_state(self, dt: float, all_tasks: List[Task]) -> None:
        """
        Updates the agent's physical state for one time step.

        Moves towards the first task in the algorithm's plan. Handles task
        locking and completion logic.

        Args:
            dt: Time step duration.
            all_tasks: List of all Task objects (for position/status lookup).
        """
        # --- 1. Get current plan from the algorithm ---
        current_plan = self.algorithm.get_plan()

        # Filter out completed tasks from the plan
        # This prevent the agent from moving towards a tasks that is already done
        while current_plan:
            target_id = current_plan[0]
            if all_tasks[target_id].completed:
                # Notify algo to cleanup this stale task from its plan
                self.algorithm.on_task_completed(target_id)
                # Re-fetch plan after update
                current_plan = self.algorithm.get_plan()
                continue
            break
            
        # If no vaild tasks, become 'IDLE'
        if not current_plan:
            self.status = 'IDLE'
            self.current_target_task_id = None
            return
        
        # --- 2. Determine target task ---
        target_task_id = current_plan[0]
        target_task = all_tasks[target_task_id]
        self.current_target_task_id = target_task_id
        
        # --- 3. Movement logic ---
        dist_to_target = np.linalg.norm(target_task.position - self.position)
        move_dist = self.speed * dt

        # Case A: En route
        if dist_to_target > move_dist and self.status != 'EXECUTING':
            self.status = 'MOVING'
            # Calculate the direction vector (uint vector)
            direction = (target_task.position - self.position) / dist_to_target
            # Updates position infomation
            self.position += direction * move_dist
            # Cumulative moving distance is used for subsequent statistics
            self.total_distance += move_dist   

        # Case B: Arrived / Executing
        else:
            if self.status != 'EXECUTING':
                # Just arrived
                # Cumulative moving distance is used for subsequent statistics
                self.total_distance += move_dist    
                self.status = 'EXECUTING'
                self.position = target_task.position
                self.time_at_task = 0.0
                # Notify algo to lock this task (prevent others from taking it)
                self.algorithm.on_task_locked(target_task_id)

            # Execute task
            self.time_at_task += dt
            if self.time_at_task >= target_task.exec_duration:
                self._complete_task(target_task)
    
    def run_algorithm_step(self, messages: List[Dict]) -> None:
        """
        Delegates the decision-making step to the algorithm strategy.
        """
        self.algorithm.run_iteration(messages)

    def prepare_message(self) -> Dict:
        """
        Delegates message packaging to the algorithm strategy.
        """
        return self.algorithm.pack_message()

    def is_idle(self) -> bool:
        """
        Checks if the agent has no active tasks and is not executing.
        """
        return self.status == 'IDLE' and not self.algorithm.get_plan()

    def _complete_task(self, task: Task) -> None:
        """
        Internal handler for task completion.
        """
        if not task.completed:
            print(f"Agent {self.id} finished Task {task.id}")
            task.completed = True
            self.completed_tasks_log.append(task.id)
            # Notify algorithm to update internal state (remove task, update bids)
            self.algorithm.on_task_completed(task.id)

        self.status = 'IDLE'
        self.time_at_task = 0.0

##### END FILE: ./core/agent.py #####

##### FILE: ./core/environment.py #####

# --------------------------------------------------------
# File: core/environment.py
# The environment for executing the simulation includes 
# agents, tasks, topology, and other components.
# --------------------------------------------------------

# The feature implementation is correct fine.

import numpy as np
from typing import Dict, List, Iterator, Any
from collections import defaultdict
from core.agent import Agent
from core.task import Task
from core.topology import Topology

# Additional rounds of information transmission
CONSENSUS_HOPS = 3

class Environment:
    """
    Simulation environment managing agents, tasks, and the simulation loop.

    This class acts as the central controller for the multi-agent system simulation.
    It handles the initialization of entities, updates the network topology based on
    agent positions, and drives the main simulation loops for both static and
    dynamic allocation modes.

    Attributes:
        agents (List[Agent]): A list of all agent instances in the simulation.
        tasks (List[Task]): A list of all task instances to be assigned or executed.
        topology (Topology): The communication graph representing connectivity between agents.
        communication_radius (float): The maximum distance between two agents to establish
            a communication link.
        fixed_topology (bool): If True, the topology will not be updated based on distance.
    """
    def __init__(self, agents: List[Agent], tasks: List[Task],
                 topology: Topology, communication_radius: float,
                 fixed_topology: bool = False) -> None:
        """
        Initializes the simulation environment.

        Args:
            agents: A list of Agent objects.
            tasks: A list of Task objects.
            topology: A Topology object managing the communication graph.
            communication_radius: The maximum distance between agents to allow communication.
            fixed_topology: If True, the topology will not be updated based on distance.
        """
        self.agents = agents
        self.tasks = tasks
        self.topology = topology
        self.communication_radius = communication_radius
        self.fixed_topology = fixed_topology  

    # *****************************************
    # Mode 1: Dynamic Simulation
    # *****************************************
    def run_dynamic_simulation(self, dt: float, max_time: float) -> Iterator[Dict[str, Any]]:
        """
        Runs a unified simulation loop where allocation and execution happen simultaneously.

        This mode simulates a dynamic environment where agents move, communicate,
        and execute tasks in real-time. It is suitable for algorithms like V1 and V2
        that support dynamic reallocation.

        Args:
            dt: The time step duration for the simulation in seconds.
            max_time: The maximum duration to run the simulation in seconds.

        Yields:
            Dict[str, Any]: A dictionary containing the current snapshot of the system state.
                Keys include 'agents', 'tasks', 'topology', 'time', and 'iteration'.
        """
        # Current simulation time
        current_time = 0.0

        # Ensure initial 'IDLE' state
        for agent in self.agents:
            agent.status = 'IDLE'

        # Initializes communication topology
        self._update_dynamic_topology()

        # Loop
        while current_time < max_time:
            # --- 1. Update topology ---
            self._update_dynamic_topology()

            # --- 2. Run algorithm ---
            # Let the information circulate a little longer
            for _ in range(CONSENSUS_HOPS):
                self._run_communication_step()

            # --- 3. Physics update ---
            all_idle = True
            for agent in self.agents:
                agent.update_state(dt, self.tasks)
                if not agent.is_idle():
                    all_idle = False
            
            # --- 4. Termination check ---
            incomplete_tasks = [t for t in self.tasks if not t.completed]
            
            # Case A: All tasks are completed, and all agents are in an idle state
            if not incomplete_tasks:
                if all_idle:
                    print(f"\n[Success] All tasks completed at {current_time:.2f}s")
                    self._print_final_log()
                    yield self.get_current_state(current_time)
                    break
            # Case B: All agents are in an idle state, but there are unfinished tasks
            elif all_idle:
                # If everyone is idle but tasks remain, it's a deadlock (impossible tasks)
                print(f"\n[Terminated] Deadlock detected at {current_time:.2f}s. "
                      f"{len(incomplete_tasks)} tasks remaining.")
                self._print_final_log()
                yield self.get_current_state(current_time)
                break

            # --- 5. Yield state for amination ---
            yield self.get_current_state(current_time)
            current_time += dt

    # *****************************************
    # Mode 2: Static Allocation (For PI - Phase 1)
    # *****************************************
    def run_static_allocation(self, max_iterations: int, convergence_threshold: int = 10) -> Iterator[Dict[str, Any]]:
        """
        Runs only the allocation algorithm without physical movement (Phase 1).

        This mode is specific to the original PI algorithm's first phase, where agents
        negotiate task assignments until consensus is reached or the iteration limit is hit.
        Agents do not move during this phase.

        Args:
            max_iterations: The maximum number of algorithm iterations to run.
            convergence_threshold: The number of consecutive stable iterations required
                to consider the allocation converged.

        Yields:
            Dict[str, Any]: A dictionary containing the current snapshot of the system state
                for visualization.
        """
        print("--- Phase 1: Static Task Allocation ---")

        # Initial topology calculation (Static)
        if self.topology.graph.number_of_edges() == 0:
             self._update_dynamic_topology()

        last_plan_signature = None
        stable_count = 0

        for i in range(max_iterations):
            # Run communication and decision making
            self._run_communication_step()

            # Check convergence
            # We construct a signature of all agents' plans to detect stability
            current_plans = tuple(tuple(a.algorithm.get_plan()) for a in self.agents)
            
            if current_plans == last_plan_signature:
                stable_count += 1
            else:
                stable_count = 0
            last_plan_signature = current_plans

            yield self.get_current_state(iteration=i)

            if stable_count >= convergence_threshold:
                print(f"Allocation converged after {i+1} iterations.")
                break
        else:
            print(f"Allocation stopped after max {max_iterations} iterations.")

    # *****************************************
    # Mode 2: Static Execution (For PI - Phase 2)
    # *****************************************
    def run_static_execution(self, dt: float, max_time: float) -> Iterator[Dict[str, Any]]:
        """
        Runs physical execution based on pre-calculated plans (Phase 2).

        This mode follows the static allocation phase. Agents execute their assigned
        tasks without further negotiation. The simulation ends when all reachable tasks
        are completed.

        Args:
            dt: The time step duration for the simulation in seconds.
            max_time: The maximum duration to run the simulation in seconds.

        Yields:
            Dict[str, Any]: A dictionary containing the current snapshot of the system state.
        """
        print("\n--- Phase 2: Mission Execution ---")
        current_time = 0.0

        # Loop
        while current_time < max_time:
            # Physics update only
            all_agents_idle = True
            for agent in self.agents:
                agent.update_state(dt, self.tasks)
                if not agent.is_idle():
                    all_agents_idle = False

            # Check if finished
            if all_agents_idle:
                completed_count = sum(1 for t in self.tasks if t.completed)
                total_count = len(self.tasks)
                if completed_count == total_count:
                    print(f"[Success] All tasks completed at {current_time:.2f}s")
                else:
                    print(f"\n[Terminated] Deadlock detected at {current_time:.2f}s. "
                      f"{total_count - completed_count}  tasks remaining.")

                # Print informations
                self._print_final_log()
                yield self.get_current_state(current_time)
                break
            # Yields state for animation
            yield self.get_current_state(current_time)
            current_time += dt
    
    # *****************************************
    # Helpers
    # *****************************************
    def _run_communication_step(self) -> None:
        """
        Executes one round of message exchange and algorithm updates.

        This helper method performs three steps:
        1. Collects messages prepared by all agents.
        2. Routes messages to neighbors based on the current topology.
        3. Triggers the algorithm step for each agent to process received messages.
        """
        # --- 1. Prepare messages for neighbors ---
        prepare_msgs = [agent.prepare_message() for agent in self.agents]
        
        # --- 2. Route messages to neighbors ---
        inbox = defaultdict(list)
        for sender in self.agents:
            neighbors = self.topology.get_neighbors(sender.id)
            for nid in neighbors:
                inbox[nid].append(prepare_msgs[sender.id])

        # --- 3. Process messages from neighbors ---
        for agent in self.agents:
            agent.run_algorithm_step(inbox[agent.id])

    def _update_dynamic_topology(self) -> None:
        """
        Recalculates the communication graph based on current agent positions.

        It computes the pairwise distances between all agents. If the distance
        is within `communication_radius`, a link is added to the adjacency matrix.
        The topology object is then updated with this new matrix.

        If `fixed_topology` is True, this method does nothing, preserving the
        manually initialized network structure.
        """
        # For original PI
        if self.fixed_topology:
            return

        # We delegate the graph building to the Topology class, 
        # but we need to provide the adjacency matrix logic here
        num_agents = len(self.agents)
        adj_matrix = [[False] * num_agents for _ in range(num_agents)]
        
        for i in range(num_agents):
            for j in range(i + 1, num_agents):
                dist = np.linalg.norm(self.agents[i].position - self.agents[j].position)
                if dist <= self.communication_radius:
                    adj_matrix[i][j] = True
                    adj_matrix[j][i] = True

        self.topology.update_from_adjacency_matrix(adj_matrix)

    def get_current_state(self, time: float = 0.0, iteration: int = 0) -> Dict[str, Any]:
        """
        Captures the current snapshot of the simulation state.

        Args:
            time: The current simulation timestamp.
            iteration: The current algorithm iteration count (for static mode).

        Returns:
            Dict[str, Any]: A dictionary containing references to agents, tasks,
                topology, and current time/iteration metadata for visualization.
        """
        return {
            "agents": self.agents,
            "tasks": self.tasks,
            "topology": self.topology,
            "time": time,
            "iteration": iteration
        }

    def _print_final_log(self):
        """
        Prints a summary log of tasks completed by each agent to the console.
        """
        print("-" * 40)
        print("Final Execution Log:")
        for agent in self.agents:
            print(f"Agent {agent.id} ({agent.agent_type}) completed: {agent.completed_tasks_log}")
        print("-" * 40)

##### END FILE: ./core/environment.py #####

##### FILE: ./core/__init__.py #####


##### END FILE: ./core/__init__.py #####

##### FILE: ./core/task.py #####

# --------------------------------------------------------
# File: core/task.py
# Implement the Task entity in the simulation.
# --------------------------------------------------------

# The feature implementation is correct fine.

import numpy as np

# Define task type constants
TASK_TYPE_MEDICINE = 'medicine'
TASK_TYPE_FOOD     = 'food'

class Task:
    """
    Represents a task in the simulation enviroment.

    Attributes:
        id (int): Unique identifier for the task.
        task_type (str): Type of the task (e.g., 'medicine', 'food').
        position (np.ndarray): The (x, y) coordinates of the task.
        deadline (float): The simulation time by which the task must be started.
        exec_duration (float): The time required to complete the task once started.
        completed (bool): Status flag indicating if the task has been finished.
    """

    def __init__(self, id: int, task_type: str, position: tuple,
                 deadline: float, exec_duration: float) -> None:
        """
        Initializes a Task instance.

        Args:
            id: Unique identifier for the task.
            task_type: Type string constant.
            position: Tuple (x, y) for spatial coordinates.
            deadline: Deadline timestamp.
            exec_duration: Duration required to execute the task.
        """
        self.id = id
        self.task_type = task_type
        self.position = np.array(position, dtype=float)
        self.deadline = deadline
        self.exec_duration = exec_duration
        self.completed = False

    def __repr__(self) -> str:
        pos_str = np.array2string(self.position, separator=',')
        return (f"Task(id={self.id}, type='{self.task_type}', "
                f"pos={pos_str}, deadline={self.deadline}, "
                f"duration={self.exec_duration}, completed={self.completed})")
    
    def __str__(self) -> str:
        return (f"Task #{self.id} ({self.task_type}) "
                f"@ {self.position}, due={self.deadline:.1f}")

##### END FILE: ./core/task.py #####

##### FILE: ./core/topology.py #####

# --------------------------------------------------------
# File: algorithms/topology.py
# Implement network communication in the simulation using 
# the NetworkX library to achieve dynamic network changes.
# --------------------------------------------------------

# The feature implementation is correct fine.

import networkx as nx
from typing import List

class Topology:
    """
    Manages the communication topology between agents using a graph structure.

    Attributes:
        graph (nx.Graph): NetworkX graph representing connectivity.
    """

    def __init__(self, agent_ids: List[int]) -> None:
        """
        Initializes the topology with a set of agent nodes.

        Args:
            agent_ids: A list of agent IDs to be added as nodes.
        """
        self.graph = nx.Graph()
        self.graph.add_nodes_from(agent_ids)

    def update_from_adjacency_matrix(self, adj_matrix: List[List[bool]]) -> None:
        """
        Rebuilds the graph edges based on an adjacency matrix.

        Args:
            adj_matrix: A square matrix where adj_matrix[i][j] is True if
                        agents i and j are connected.
        """
        self.graph.clear_edges()
        num_agents = len(adj_matrix)
        for i in range(num_agents):
            for j in range(i + 1, num_agents):
                if adj_matrix[i][j]:
                    self.graph.add_edge(i, j)

    def is_connected(self) -> bool:
        """
        Checks if the entire graph is connected.

        Returns:
            True if the graph is connected, False otherwise.
        """
        if self.graph.number_of_nodes() == 0:
            return True
        return nx.is_connected(self.graph)
    
    def get_agent_count(self) -> int:
        """
        Returns the total number of agents in the topology.
        """
        return self.graph.number_of_nodes()

    def get_neighbors(self, agent_id: int) -> List[int]:
        """
        Retrieves the IDs of neighbors connected to a specific agent.

        Args:
            agent_id: The ID of the agent query.

        Returns:
            A list of neighbor agent IDs. Returns an empty list if the agent
            is not in the graph.
        """
        if agent_id not in self.graph:
            return []
        return list(self.graph.neighbors(agent_id))

##### END FILE: ./core/topology.py #####

##### FILE: ./main.py #####

# ------------------------------
# File: main.c
# ------------------------------

import re
import os
import sys
import random
import argparse
import matplotlib.pyplot as plt
from typing import Tuple, List, Dict

from core.agent import Agent, AGENT_TYPE_MEDICINE, AGENT_TYPE_FOOD
from core.task import Task, TASK_TYPE_MEDICINE, TASK_TYPE_FOOD
from core.topology import Topology
from core.environment import Environment
from algorithms.factory import AlgorithmFactory
from visualization.animator import Animator

# Defaults
NUM_AGENTS = 6 
NUM_TASKS = 12 
NETWORK = 'row'
MAX_TASKS = 3
MAX_TIME = 5000.0
FIELD_SIZE = 2000.0
MAX_ITER = 100
RADIUS = 300.0 
DT = 2.0 

# ********************************** 
# IO Helpers (Ported from V2)
# ********************************** 
def ensure_data_prefix(filename):
    if not os.path.dirname(filename):
        return os.path.join('data', filename)
    else:
        return filename

def save_info_to_file(tasks: List[Task], agents: List[Agent], filename: str):
    """
    Saves the current simulation configuration to a text file.

    Writes task and agent positions, deadlines, and execution durations to the
    specified file path. This allows for reproducing specific random scenarios.

    Args:
        tasks: A list of Task objects present in the simulation.
        agents: A list of Agent objects present in the simulation.
        filename: The target file path to write the configuration to.

    Raises:
        IOError: If the file cannot be opened or written to.
    """
    try:
        with open(filename, 'w') as f:
            f.write('--- Tasks Info ---\n')
            for task in tasks:
                f.write(f"pos: ({task.position[0]}, {task.position[1]}), "
                        f"deadline: {task.deadline}, "
                        f"duration: {task.exec_duration}\n")

            f.write('--- Agents Info ---\n')
            for agent in agents:
                f.write(f"pos: ({agent.position[0]}, {agent.position[1]})\n")
        print(f"Successfully saved scenario to '{filename}'.")
    except IOError as e:
        print(f"Error saving file: {e}", file=sys.stderr)

def load_info_from_file(filename: str) -> Tuple[List[Dict], List[Dict]]:
    """
    Parses simulation configuration from a saved text file.

    Reads a file generated by `save_info_to_file` and extracts task and agent
    initialization parameters using regular expressions.

    Args:
        filename: The path to the configuration file.

    Returns:
        A tuple containing two lists:
            - task_infos (List[Dict]): A list of dictionaries, each containing
              'pos', 'deadline', and 'duration' for a task.
            - agent_infos (List[Dict]): A list of dictionaries, each containing
              'pos' for an agent.

    Raises:
        SystemExit: If the file cannot be found or read (exits the program).
    """
    task_infos = []
    agent_infos = []
    mode = ''
    
    # Regex patterns
    task_pat = r'pos:\s*\(\s*(\d+\.?\d*),\s*(\d+\.?\d*)\s*\),\s*deadline:\s*(\d+\.?\d*),\s*duration:\s*(\d+\.?\d*)'
    agent_pat = r'pos:\s*\(\s*(\d+\.?\d*),\s*(\d+\.?\d*)\s*\)'

    try:
        with open(filename, 'r') as f:
            for line in f:
                line = line.strip()
                if line == "--- Tasks Info ---":
                    mode = 'task'
                    continue
                elif line == "--- Agents Info ---":
                    mode = 'agent'
                    continue

                if mode == 'task':
                    m = re.search(task_pat, line)
                    if m:
                        task_infos.append({
                            'pos': (float(m.group(1)), float(m.group(2))),
                            'deadline': float(m.group(3)),
                            'duration': float(m.group(4))
                        })
                elif mode == 'agent':
                    m = re.search(agent_pat, line)
                    if m:
                        agent_infos.append({
                            'pos': (float(m.group(1)), float(m.group(2)))
                        })
        print(f"Successfully loaded {len(task_infos)} tasks and {len(agent_infos)} agents from '{filename}'.")
    except IOError as e:
        print(f"Error loading file: {e}", file=sys.stderr)
        sys.exit(1)
        
    return task_infos, agent_infos

def setup_manual_network(adj_matrix: List[List[bool]], num_agents: int, net_type: str) -> None:
    """
    Sets up the adjacency matrix based on a fixed topology pattern.
    """
    if net_type == 'full':
        for i in range(num_agents):
            for j in range(i + 1, num_agents):
                adj_matrix[i][j] = adj_matrix[j][i] = True
    
    elif net_type == 'row':
        # A-B-C-D-E
        for i in range(num_agents - 1):
            adj_matrix[i][i+1] = adj_matrix[i+1][i] = True
            
    elif net_type == 'star':
        # Center is 0
        for i in range(1, num_agents):
            adj_matrix[0][i] = adj_matrix[i][0] = True
            
    elif net_type == 'circle':
        # Ring
        for i in range(num_agents - 1):
            adj_matrix[i][i+1] = adj_matrix[i+1][i] = True
        adj_matrix[0][num_agents-1] = adj_matrix[num_agents-1][0] = True

# ********************************** 
# Scenario Setup
# ********************************** 
def _create_agent_objects(agent_data: List[Dict], args, tasks: List[Task]) -> List[Agent]:
    """
    Instantiates Agent objects and injects the selected algorithm strategy.

    This helper iterates through agent initialization data, assigns types (Medicine/Food)
    based on index, creates the specified algorithm instance via the factory, and
    binds the agent to the algorithm.

    Args:
        agent_data: A list of dictionaries containing initial positions for agents.
        args: Parsed command-line arguments containing simulation parameters.
        tasks: The global list of tasks (required for algorithm initialization).

    Returns:
        List[Agent]: A list of fully initialized Agent objects.
    """
    agents = []
    cmap = plt.get_cmap('viridis')
    num_agents = len(agent_data)

    for i, info in enumerate(agent_data):
        # Determine properties based on index (Medicine vs Food)
        if i < num_agents / 2:
            a_type = AGENT_TYPE_MEDICINE
            speed = 30.0
        else:
            a_type = AGENT_TYPE_FOOD
            speed = 50.0
        
        color = cmap(i / (num_agents - 1)) if num_agents > 1 else cmap(0.5)
        pos = info['pos']

        # Create Algorithm Strategy
        algo = AlgorithmFactory.create(args.algorithm, i, args.max_tasks_per_agent, tasks)
        
        # Create Agent
        new_agent = Agent(
            id=i, agent_type=a_type, position=pos, speed=speed, 
            color=color, algorithm=algo
        )
        
        # Bind agent back to algorithm
        algo.bind_agent(new_agent)
        agents.append(new_agent)
        
    return agents

def setup_scenario(args) -> Tuple[List[Task], List[Agent], Topology]:
    """
    Sets up the complete simulation scenario including tasks, agents, and topology.

    This function handles two modes of initialization:
    1. Random Generation: Creates tasks and agents with random positions if no load file is specified.
    2. File Loading: Reconstructs the scenario from a saved configuration file.

    It also handles the 'save' functionality if requested.

    Args:
        args: Parsed command-line arguments.

    Returns:
        Tuple[List[Task], List[Agent], Topology]: A tuple containing the lists of
        created tasks and agents, and the initialized topology object.
    """
    tasks = []
    agent_data_list = [] # Intermediate storage for agent positions

    # --- Branch 1: Load from File ---
    if args.load:
        t_infos, a_infos = load_info_from_file(args.file)
        
        # 1. Reconstruct Tasks
        for i, info in enumerate(t_infos):
            t_type = TASK_TYPE_MEDICINE if i < len(t_infos) / 2 else TASK_TYPE_FOOD
            tasks.append(Task(
                id=i, task_type=t_type, position=info['pos'],
                deadline=info['deadline'], exec_duration=info['duration']
            ))
        
        # 2. Prepare Agent Data
        agent_data_list = a_infos

    # --- Branch 2: Generate Random ---
    else:
        # 1. Generate Tasks
        for i in range(args.tasks):
            t_type = TASK_TYPE_MEDICINE if i < args.tasks / 2 else TASK_TYPE_FOOD
            tasks.append(Task(
                id=i, task_type=t_type,
                position=(random.uniform(0, FIELD_SIZE), random.uniform(0, FIELD_SIZE)),
                deadline=random.uniform(0, 2000),
                exec_duration=random.uniform(50, 100)
            ))
        
        # 2. Generate Agent Positions
        for i in range(args.agents):
            pos = (random.uniform(0, FIELD_SIZE), random.uniform(0, FIELD_SIZE))
            agent_data_list.append({'pos': pos})

    # --- Common: Instantiate Agents & Algorithms ---
    # Note: We need 'tasks' to exist before creating algorithms
    agents = _create_agent_objects(agent_data_list, args, tasks)

    # --- Save if requested (and not loading) ---
    if args.save and not args.load:
        filename = ensure_data_prefix(args.file)
        save_info_to_file(tasks, agents, filename)

    # --- Common: Finalize Setup ---
    # 1. Setup Feasibility (Global Info)
    feasibility = [[False] * len(tasks) for _ in range(len(agents))]
    for i, agent in enumerate(agents):
        for j, task in enumerate(tasks):
            if agent.agent_type == task.task_type:
                feasibility[i][j] = True
    
    for agent in agents:
        agent.algorithm.finalize_setup(feasibility)

    # 2. Topology
    topology = Topology([a.id for a in agents])

    if args.network != 'radius':
        num = len(agents)
        adj = [[False] * num for _ in range(num)]
        setup_manual_network(adj, num, args.network)
        topology.update_from_adjacency_matrix(adj)
        print(f">>> Topology initialized as '{args.network}'")
    else:
        print(f">>> Topology will be calculated by Radius ({args.radius})")
    
    return tasks, agents, topology

def parse_args():
    """
    Parses command-line arguments.

    Defines and parses all available flags for simulation configuration, algorithm
    selection, and input/output control.

    Returns:
        argparse.Namespace: An object containing the values of all command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Unified MRTA Simulation Framework")
    
    # Algorithm Selection
    parser.add_argument("--algo", dest="algorithm", type=str, default="v2", 
                        choices=["original", "dynamic"], help="Algorithm strategy to use.")
    
    # Simulation Config
    parser.add_argument("-a", "--agents", type=int, default=NUM_AGENTS, help="Number of agents")
    parser.add_argument("-t", "--tasks", type=int, default=NUM_TASKS, help="Number of tasks")
    parser.add_argument("-r", "--radius", type=float, default=RADIUS, help="Comm radius")
    parser.add_argument("-m", "--max-tasks-per-agent", type=int, default=MAX_TASKS, help="Capacity constraint")
    parser.add_argument("-i", "--max-iterations", type=int, default=MAX_ITER, help="Max iterations for static allocation")
    parser.add_argument("--network", type=str, default='row', choices=['radius', 'row', 'star', 'circle', 'full'], help="Network topology type.")
    
    # IO & Control Flags
    parser.add_argument("--save", action="store_true", help="Save scenario to file")
    parser.add_argument("--load", action="store_true", help="Load scenario from file")
    parser.add_argument("--file", type=str, default="scenario_info.txt", help="Filename for save/load")
    parser.add_argument("--quiet", action="store_true", help="Run without visualization (Console only)")

    return parser.parse_args()

# ********************************** 
# Main Entry
# ********************************** 
if __name__ == "__main__":
    args = parse_args()
    
    # 1. Setup
    tasks, agents, topology = setup_scenario(args)
    is_fixed_topology = (args.network != 'radius')
    env = Environment(
        agents, 
        tasks, 
        topology, 
        args.radius, 
        fixed_topology=is_fixed_topology 
    )

    # 2. Select Generator Mode
    if args.algorithm == "original":
        print(">>> Mode: Static Allocation + Execution (Original PI)")

        def chained_generator():
            # Phase 1
            yield from env.run_static_allocation(args.max_iterations)
            # Phase 2
            yield from env.run_static_execution(DT, MAX_TIME)
        generator = chained_generator()
        
    else: # v1 or v2
        print(f">>> Mode: Dynamic Simulation ({args.algorithm})")
        generator = env.run_dynamic_simulation(DT, MAX_TIME)

    # 3. Run
    if args.quiet:
        print("Running in quiet mode...")
        for _ in generator: 
            pass
        print("Simulation finished.")
    else:
        # Pass environment to animator for access to static info if needed
        animator = Animator(env, generator, args.radius)
        animator.run()

##### END FILE: ./main.py #####

##### FILE: ./visualization/animator.py #####

# --------------------------------------------------------
# File: visualization/animator.py
# Perform an animated demonstration of the simulation.
# --------------------------------------------------------

# The feature implementation is correct fine.

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.patches import Circle
from matplotlib.lines import Line2D

class Animator:
    """
    Handles visualization for both static and dynamic multi-agent simulations.

    This class is responsible for creating and managing the matplotlib animation
    loop. It supports rendering tasks, agents, communication links (topology),
    and agent paths for both static (two-phase) and dynamic (real-time) modes.

    Attributes:
        env: The simulation Environment object containing entity data.
        generator (Generator): The simulation data generator yielding state snapshots.
        comm_radius (float): The communication radius for visualizing coverage circles.
        fig (plt.Figure): The matplotlib figure object.
        ax (plt.Axes): The matplotlib axes object.
        dynamic_artists (List[plt.Artist]): A list of artists (plot elements) drawn
            in the current frame, used for efficient clearing/updating.
        colors (Dict[str, str]): Mapping of task types to colors.
        markers (Dict[str, str]): Mapping of agent types to plot markers.
    """

    def __init__(self, environment, generator, comm_radius):
        """
        Initializes the Animator with simulation context.

        Args:
            environment: The simulation Environment instance.
            generator: A generator function yielding state dictionaries.
            comm_radius: The visual radius for agent communication circles.
        """
        # Basic component
        self.env = environment
        self.generator = generator
        self.comm_radius = comm_radius

        # Setup figure
        self.fig, self.ax = plt.subplots(figsize=(10, 10)) # Adjusted size
        self.dynamic_artists = []
        
        # Visual fonfig
        self.colors = {'medicine': 'red', 'food': 'green'}
        self.markers = {'medicine': '^', 'food': 'P'}

    def _init_plot(self):
        """
        Initializes the static background elements of the plot.

        Sets up axis limits, titles, and static legends. This function is called
        once at the start of the animation.

        Returns:
            List[plt.Artist]: An empty list as no dynamic artists are drawn initially.
        """
        self.ax.clear()
        self.ax.set_xlim(0, 2000)
        self.ax.set_ylim(0, 2000)
        self.ax.set_title("Initializing...")
        # Add legends
        legend_elements = [
            Line2D([0], [0], marker='^', color='w', markerfacecolor='gray', label='Agent (Medicine)', markersize=10),
            Line2D([0], [0], marker='P', color='w', markerfacecolor='gray', label='Agent (Food)', markersize=10),
            Line2D([0], [0], marker='o', color='w', markerfacecolor='red', label='Task (Medicine)', markersize=8),
            Line2D([0], [0], marker='o', color='w', markerfacecolor='green', label='Task (Food)', markersize=8),
        ]
        self.ax.legend(handles=legend_elements, loc='best')
        
        return []

    def _update(self, frame):
        """
        Updates the plot for a new animation frame.

        Retrieves the next state from the simulation generator, clears old dynamic
        elements, and redraws agents, tasks, topology, and paths.

        Args:
            frame: The current frame index (automatically passed by FuncAnimation).

        Returns:
            List[plt.Artist]: A list of all dynamic artists drawn in this frame.
        """
        try:
            # Retrieve the physical state of each frame in the simulation
            state = next(self.generator)
        except StopIteration:
            # Stop animation if generator is exhausted
            return []

        # Clear previous dynamic elements (but keep static background)
        for artist in self.dynamic_artists:
            artist.remove()
        self.dynamic_artists = []

        # --- 1. Draw Tasks ---
        # Only draw uncompleted tasks to visualize progress
        for task in state['tasks']:
            if not task.completed:
                # Draw Task point
                sc = self.ax.scatter(task.position[0], task.position[1], 
                                     c=self.colors[task.task_type], s=100, zorder=3)
                # Draw Task ID label
                tx = self.ax.text(task.position[0], task.position[1]-40, f"T{task.id}", ha='center')
                self.dynamic_artists.extend([sc, tx])

        # --- 2. Draw topology ---
        # Draw edges between agents if they are connected in the graph
        pos = {a.id: a.position for a in state['agents']}
        edges = nx.draw_networkx_edges(state['topology'].graph, pos=pos, ax=self.ax, 
                                       edge_color='gray', style='--', alpha=0.5)
        if edges:
            self.dynamic_artists.append(edges)

        # --- 3. Draw Agents & paths ---
        for agent in state['agents']:
            # Draw communication range (circle)
            c = Circle(agent.position, self.comm_radius, color='gray', alpha=0.1)
            self.ax.add_patch(c)
            self.dynamic_artists.append(c)
            
            # Draw agent body
            sc = self.ax.scatter(agent.position[0], agent.position[1], 
                                 color=agent.color, marker=self.markers[agent.agent_type], s=150, zorder=5, edgecolors='black')
            # Draw Agent ID label
            tx = self.ax.text(agent.position[0], agent.position[1]+40, f"A{agent.id}", ha='center')
            self.dynamic_artists.extend([sc, tx])

            # Draw planned path
            # Fetches the current intended path directly from the algorithm interface
            plan = agent.algorithm.get_plan()
            if plan:
                # Construct path points: [Current Pos] -> [Task 1] -> [Task 2] ...
                pts = [agent.position] + [self.env.tasks[tid].position for tid in plan]
                pts = np.array(pts)
                ln, = self.ax.plot(pts[:,0], pts[:,1], c=agent.color, ls='-', lw=1, alpha=0.6)
                self.dynamic_artists.append(ln)

        # --- 4. Update title ---
        t_str = f"Time: {state['time']:.1f}s"
        if 'iteration' in state and state['iteration'] > 0:
            # For static allocation phase, show iteration count
            t_str = f"Allocation Iter: {state['iteration']}"
        
        self.ax.set_title(f"MRTA Simulation | {t_str}")

        return self.dynamic_artists

    def run(self):
        """
        Starts the matplotlib animation loop.

        Configures and runs `FuncAnimation`. The plot window will block execution
        until closed.
        """
        _ = animation.FuncAnimation(
            self.fig, 
            self._update, 
            init_func=self._init_plot, 
            interval=50,       # Update every 50ms
            blit=False,        # Turn off blitting for compatibility with dynamic artists
            save_count=500     # Max frames to cache if saving
        )
        plt.show()

##### END FILE: ./visualization/animator.py #####

##### FILE: ./visualization/__init__.py #####


##### END FILE: ./visualization/__init__.py #####

